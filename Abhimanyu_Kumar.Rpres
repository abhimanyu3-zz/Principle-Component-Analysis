
========================================================
author: Abhimanyu Kumar
date: Feb 10th, 2018
autosize: true

========================================================

Principal component Analysis.

I have taken data set from an ongoing hackathon and implemented it with the help of some reference.

PCA <- When we have too many data sets we always get confused with the idea that most of them are corelated and PCA helps us in dealing with such situation.
Basically its a method of extracting important variables from a set of large no. variables in a dataset. It is even more powerful when dealing with 3 or higher dimensional data.

PCA can only be applied on numerical data so if our data set is having categorical then we should convert them into numerical data.

Some basic data cleaning is also important before implementing the technique to improve its accuracy.

========================================================

```{r}
#load train and test file
train <- read.csv("train_Big.csv")
test <- read.csv("test_Big.csv")

#add a column
test$Item_Outlet_Sales <- 1

#combine the data set
combi <- rbind(train, test)

#impute missing values with median
combi$Item_Weight[is.na(combi$Item_Weight)] <- median(combi$Item_Weight, na.rm = TRUE)

#impute 0 with median
combi$Item_Visibility <- ifelse(combi$Item_Visibility == 0, median(combi$Item_Visibility),combi$Item_Visibility)

#find mode and impute
table(combi$Outlet_Size, combi$Outlet_Type)
levels(combi$Outlet_Size)[1] <- "Other"
```

========================================================

As this is a unsupervised technique so we need to remove dependent and other identifier variable. We will do that in the below steps.

```{r}
#remove the dependent and identifier variables
my_data <- subset(combi, select = -c(Item_Outlet_Sales, Item_Identifier,Outlet_Identifier))

```

Let’s check the available variables ( a.k.a predictors) in the data set.
```{r}
#check available variables
 colnames(my_data)
```

========================================================
Since PCA works on numeric variables, let’s see if we have any variable other than numeric.
```{r}
#check variable class
str(my_data)
```

We can see here that 6 out 9 variables we have are categorical. So, we will convert them into numeric variables in our further steps.

========================================================

I installed a package named dummies to create dummy data as i used one hot encoding to get rid of the categorical data. Further after that i created a dummy data frame with one column by encoding categorical data.

```{r}
#install package
install.packages("dummies", repos = "http://cran.us.r-project.org")
#load library
library(dummies)
```

```{r}
#create a dummy data frame
new_my_data <- dummy.data.frame(my_data, names = c("Item_Fat_Content","Item_Type",
                                "Outlet_Establishment_Year","Outlet_Size",
                                "Outlet_Location_Type","Outlet_Type"))
```

========================================================

To check, if we now have a data set of integer values, simple write:

```{r}
#check the data set
str(new_my_data)
```

========================================================

Now we have all the dataset into numerical value so again i will divide them into two parts in which they originally were i.e. test and train.

```{r}
#divide the new data
pca.train <- new_my_data[1:nrow(train),]
pca.test <- new_my_data[-(1:nrow(train)),]
```

## Now, I am all set to apply PCA here. 

========================================================

We have 5 function to perform PCA but Here I will be using base R function prcomp() to perform PCA. It is having a bydefault propert that it centers the variable to have mean equals to zero. We will also normalize the variables to have standard deviation equals to 1 with parameter scale = T.

```{r}
#principal component analysis
prin_comp <- prcomp(pca.train, scale. = T)
names(prin_comp)
```

========================================================

The prcomp() function results in 5 useful measures, which i will be explainign one by one in my further steps :-

## Step 1.) Here center and scale refers to respective mean and standard deviation of the variables that are used for normalization prior to implementing PCA.

## Mean
```{r}
#outputs the mean of variables
prin_comp$center
```

========================================================
## Standard deviation.
```{r}
#outputs the standard deviation of variables
prin_comp$scale
```

========================================================

## Step 2.) The rotation measure provides the principal component loading. Each column of rotation matrix contains the principal component loading vector. This is the most important measure we should be interested in.

```{r}
prin_comp$rotation
```

========================================================

We got 44 principa components loadings. The question is, if we got correct result or not. As per fact, In a data set, the maximum no of principal component is a minimum of (n-1,p). To verify this lets look at the first 4 principal components and first 5 rows.

```{r}
prin_comp$rotation[1:5,1:4]
```

## So, We are absolutely right here.

========================================================

## Step 3.) In order to compute the principal component score vector, we don’t need to multiply the loading with data. Rather, the matrix x has the principal component score vectors in a 8523 × 44 dimension.

```{r}
dim(prin_comp$x)
```


========================================================
Let’s plot the resultant principal components.

```{r}
biplot(prin_comp, scale = 0)
```

To ensure that arrows are scaled to represent the loadings I gave value of parameter scale =0.
After focusing on the extreme ends of the graph we can infer that first PC corresponds to a measure of Outlet_TypeSupermarket, Outlet_Establishment_Year 2007. Similarly, it can be said that the second component corresponds to a measure of Outlet_Location_TypeTier1, Outlet_Sizeother. For exact measure of a variable in a component, we need to look at rotation matrix(above) again.

========================================================

## Step 4.) From the prcomp() function we can compute the standard deviation of each principal component. Here, sdev refers to the standard deviation of principal components.

## standard deviation
```{r}
#compute standard deviation of each principal component
std_dev <- prin_comp$sdev
```

## Variance
```{r}
#compute variance
pr_var <- std_dev^2
```

```{r}
#check variance of first 10 components
pr_var[1:10]
```

Here my main aim is to find the components which expains the maximum variance because I want to retain as much information as possible by using these components. We can infer that the higher is the explained variance,higher will be the information containe din those components.

========================================================

To compute the proportion of variance explained by each component, I simply divide the variance by sum of total variance. 
```{r}
#proportion of variance explained
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:20]
```

This shows that first principal component explains 10.3% variance. Second component explains 7.3% variance. Third component explains 6.2% variance and so on. So, how do we decide how many components should I select for modeling stage ?

========================================================

The answer to this question is provided by a scree plot. A scree plot is used to access components or factors which explains the most of variability in the data. It represents values in descending order.

```{r}
#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
```

The plot above shows that ~ 30 components explains around 98.4% variance in the data set. In order words, using PCA we have reduced 44 predictors to 30 without compromising on explained variance. This is the power of PCA> Let’s do a confirmation check, by plotting a cumulative variance plot. This will give us a clear picture of number of components.

========================================================

```{r}
#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```

This plot shows that 30 components results in variance close to ~ 98%. Therefore, in this case, we’ll select number of components as 30 [PC1 to PC30] and proceed to the modeling stage. This completes the steps to implement PCA on train data. For modeling, I will use these 30 components as predictor variables and follow the normal procedures.

========================================================

## Predictive Modeling with PCA Components

As I have calculated the PC on the training sets, so we can now get on the process of predicting on test data using these components. 

So, what should I do?

I will do exactly the same transformation to the test set as what I did to training set, including the center and scaling feature.

```{r}
#add a training set with principal components
train.data <- data.frame(Item_Outlet_Sales = train$Item_Outlet_Sales, prin_comp$x)
```

```{r}
#I am interested in first 30 PCAs
train.data <- train.data[,1:31]
```

========================================================

```{r}
#run a decision tree
install.packages("rpart", repos = "http://cran.us.r-project.org")
library(rpart)
rpart.model <- rpart(Item_Outlet_Sales ~ .,data = train.data, method = "anova")
rpart.model
```

========================================================
```{r}
#transform test into PCA
test.data <- predict(prin_comp, newdata = pca.test)
test.data <- as.data.frame(test.data)
```

```{r}
#select the first 30 components
test.data <- test.data[,1:30]

#make prediction on test data
rpart.prediction <- predict(rpart.model, test.data)

#For fun, finally check your score of leaderboard
sample <- read.csv("sample.csv")
final.sub <- data.frame(Item_Identifier = sample$Item_Identifier, Outlet_Identifier = sample$Outlet_Identifier, Item_Outlet_Sales = rpart.prediction)

head(final.sub)
# To get the whole predicted data.
#write.csv(final.sub, "pca.csv",row.names = F)
```
========================================================

